{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c90b85",
   "metadata": {},
   "source": [
    "# Game Bug Classifier - Complete Pipeline\n",
    "## Label Improvement â†’ Training â†’ Evaluation\n",
    "\n",
    "**Requirements:**\n",
    "- Kaggle GPU enabled\n",
    "- Internet enabled\n",
    "- Input dataset: `data-splits` (train.jsonl, val.jsonl, test.jsonl)\n",
    "\n",
    "**Runtime:** ~5-6 hours total\n",
    "- Label improvement: ~3 hours\n",
    "- Training: ~2 hours  \n",
    "- Evaluation: ~5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03630f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers==4.46.3 peft==0.13.2 bitsandbytes==0.45.0 accelerate==1.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de380472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify paths\n",
    "!ls /kaggle/input/data-splits/\n",
    "!echo \"\\nWorking directory:\"\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e5428d",
   "metadata": {},
   "source": [
    "## Step 1: Label Improvement with Mistral Base Model\n",
    "\n",
    "Uses the instruction-tuned Mistral-7B base model to generate better labels than keyword-based heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b24d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_dataset(path):\n",
    "    \"\"\"Load JSONL dataset\"\"\"\n",
    "    data = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def improve_labels_with_mistral(bug_report, model, tokenizer):\n",
    "    \"\"\"Generate improved labels using Mistral base model\"\"\"\n",
    "    instruction = \"\"\"Classify this game engine bug report. Provide labels in exactly this format:\n",
    "Severity: [critical/major/moderate/minor]\n",
    "Component: [rendering/physics/ui/networking/audio/gameplay/other]\n",
    "Reproducibility: [always/often/sometimes/rare]\n",
    "\n",
    "Guidelines:\n",
    "- critical: crashes, data loss, security, game-breaking\n",
    "- major: broken features, significant gameplay impact\n",
    "- moderate: noticeable issues with workarounds\n",
    "- minor: cosmetic, polish, minor annoyances\n",
    "\n",
    "- always: 100% reproduction\n",
    "- often: >70% reproduction  \n",
    "- sometimes: 30-70% reproduction\n",
    "- rare: <30% reproduction or unclear steps\n",
    "\n",
    "Base classification on context and impact, not just keywords.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{bug_report}\n",
    "\n",
    "### Output:\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=80,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        return generated.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Configuration\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "TRAIN_PATH = \"/kaggle/input/data-splits/train.jsonl\"\n",
    "OUTPUT_PATH = \"/kaggle/working/train_improved.jsonl\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ”¬ LABEL QUALITY IMPROVEMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load training data\n",
    "print(f\"\\nðŸ“‚ Loading training data...\")\n",
    "train_data = load_dataset(TRAIN_PATH)\n",
    "print(f\"âœ… Loaded {len(train_data)} examples\")\n",
    "\n",
    "# Load Mistral base model\n",
    "print(f\"\\nðŸ” Loading Mistral-7B-Instruct base model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()\n",
    "print(\"âœ… Model loaded\")\n",
    "\n",
    "# Improve labels\n",
    "print(f\"\\nðŸš€ Generating improved labels...\")\n",
    "improved_data = []\n",
    "failed = 0\n",
    "\n",
    "for example in tqdm(train_data, desc=\"Improving labels\"):\n",
    "    improved_output = improve_labels_with_mistral(example['input'], model, tokenizer)\n",
    "    \n",
    "    if improved_output:\n",
    "        improved_example = {\n",
    "            'instruction': example['instruction'],\n",
    "            'input': example['input'],\n",
    "            'output': improved_output,\n",
    "            'metadata': {\n",
    "                'original_output': example['output'],\n",
    "                'improvement_method': 'mistral-base-model'\n",
    "            }\n",
    "        }\n",
    "        improved_data.append(improved_example)\n",
    "    else:\n",
    "        failed += 1\n",
    "        improved_data.append(example)\n",
    "\n",
    "# Save improved dataset\n",
    "print(f\"\\nðŸ’¾ Saving improved dataset...\")\n",
    "with open(OUTPUT_PATH, 'w') as f:\n",
    "    for example in improved_data:\n",
    "        f.write(json.dumps(example) + '\\n')\n",
    "\n",
    "print(f\"âœ… Saved {len(improved_data)} examples\")\n",
    "print(f\"âš ï¸  Failed: {failed} examples\")\n",
    "\n",
    "# Show comparison samples\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š SAMPLE COMPARISON (First 2)\")\n",
    "print(\"=\"*70)\n",
    "for i in range(min(2, len(improved_data))):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Input: {improved_data[i]['input'][:100]}...\")\n",
    "    print(f\"\\nOriginal: {improved_data[i]['metadata']['original_output']}\")\n",
    "    print(f\"\\nImproved: {improved_data[i]['output']}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"\\nâœ¨ Label improvement complete!\")\n",
    "\n",
    "# Clean up to free memory\n",
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47629092",
   "metadata": {},
   "source": [
    "## Step 2: Train Model with Improved Labels\n",
    "\n",
    "Fine-tune Mistral-7B with QLoRA on the improved labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcccdae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "TRAIN_PATH = \"/kaggle/working/train_improved.jsonl\"\n",
    "VAL_PATH = \"/kaggle/input/data-splits/val.jsonl\"\n",
    "OUTPUT_DIR = \"/kaggle/working/outputs\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸŽ® TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load data\n",
    "print(\"\\nðŸ“‚ Loading datasets...\")\n",
    "train_data = load_dataset(TRAIN_PATH)\n",
    "val_data = load_dataset(VAL_PATH)\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)}\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"\\nðŸ¤– Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Setup LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Tokenize function\n",
    "def format_and_tokenize(example):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Input:\n",
    "{example['input']}\n",
    "\n",
    "### Output:\n",
    "{example['output']}\"\"\"\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"\\nðŸ”¤ Tokenizing...\")\n",
    "train_dataset = train_dataset.map(\n",
    "    format_and_tokenize,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing training data\"\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    format_and_tokenize,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation data\"\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\\nðŸš€ Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "print(\"\\nðŸ’¾ Saving final model...\")\n",
    "final_output_dir = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "trainer.save_model(final_output_dir)\n",
    "tokenizer.save_pretrained(final_output_dir)\n",
    "\n",
    "print(f\"\\nâœ… Training complete! Model saved to: {final_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf3c78",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate Model\n",
    "\n",
    "Test the fine-tuned model on held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c77e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"/kaggle/working/outputs/final_model\"\n",
    "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "TEST_PATH = \"/kaggle/input/data-splits/test.jsonl\"\n",
    "NUM_SAMPLES = 50\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸŽ¯ EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load test data\n",
    "print(f\"\\nðŸ“‚ Loading test data...\")\n",
    "test_data = load_dataset(TEST_PATH)\n",
    "test_data = test_data[:NUM_SAMPLES]\n",
    "print(f\"âœ… Testing on {len(test_data)} examples\")\n",
    "\n",
    "# Load model\n",
    "print(f\"\\nðŸ” Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, MODEL_PATH)\n",
    "model.eval()\n",
    "print(\"âœ… Model loaded\")\n",
    "\n",
    "# Generate predictions\n",
    "def generate_prediction(model, tokenizer, instruction, input_text):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Output:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return generated.strip()\n",
    "\n",
    "def parse_output(text):\n",
    "    fields = {}\n",
    "    for field in ['severity', 'component', 'reproducibility']:\n",
    "        key = field.capitalize() + \":\"\n",
    "        if key in text:\n",
    "            start = text.index(key) + len(key)\n",
    "            end = text.find(\"\\n\", start)\n",
    "            if end == -1:\n",
    "                end = len(text)\n",
    "            fields[field] = text[start:end].strip().lower()\n",
    "    return fields\n",
    "\n",
    "print(\"\\nðŸš€ Generating predictions...\")\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "for example in tqdm(test_data, desc=\"Evaluating\"):\n",
    "    pred = generate_prediction(model, tokenizer, example['instruction'], example['input'])\n",
    "    predictions.append(pred)\n",
    "    ground_truths.append(example['output'])\n",
    "\n",
    "# Calculate accuracy\n",
    "def calculate_accuracy(predictions, ground_truths, field):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for pred, gt in zip(predictions, ground_truths):\n",
    "        pred_fields = parse_output(pred)\n",
    "        gt_fields = parse_output(gt)\n",
    "        \n",
    "        if field in pred_fields and field in gt_fields:\n",
    "            if pred_fields[field] == gt_fields[field]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    return (correct / total * 100) if total > 0 else 0.0\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "severity_acc = calculate_accuracy(predictions, ground_truths, 'severity')\n",
    "component_acc = calculate_accuracy(predictions, ground_truths, 'component')\n",
    "repro_acc = calculate_accuracy(predictions, ground_truths, 'reproducibility')\n",
    "overall = (severity_acc + component_acc + repro_acc) / 3\n",
    "\n",
    "print(f\"\\nSeverity:       {severity_acc:.2f}%\")\n",
    "print(f\"Component:      {component_acc:.2f}%\")\n",
    "print(f\"Reproducibility: {repro_acc:.2f}%\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Overall:        {overall:.2f}%\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    \"metrics\": {\n",
    "        \"severity_accuracy\": severity_acc,\n",
    "        \"component_accuracy\": component_acc,\n",
    "        \"reproducibility_accuracy\": repro_acc,\n",
    "        \"overall_average\": overall\n",
    "    },\n",
    "    \"num_samples\": len(test_data)\n",
    "}\n",
    "\n",
    "with open(\"/kaggle/working/evaluation_results.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete!\")\n",
    "print(\"Results saved to: /kaggle/working/evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f686b30a",
   "metadata": {},
   "source": [
    "## Step 4: Download Results\n",
    "\n",
    "Download the trained model and evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c5e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the final model for download\n",
    "!cd /kaggle/working && zip -r bug_classifier_model_v2.zip outputs/final_model/\n",
    "\n",
    "print(\"\\nâœ… Model zipped!\")\n",
    "print(\"\\nDownload these files:\")\n",
    "print(\"- /kaggle/working/bug_classifier_model_v2.zip\")\n",
    "print(\"- /kaggle/working/evaluation_results.json\")\n",
    "print(\"- /kaggle/working/train_improved.jsonl (optional - for analysis)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
